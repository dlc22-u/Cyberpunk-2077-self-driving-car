{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 0.38512692917352437, Time: 492.85 seconds\n",
      "Epoch 2/50, Loss: 0.36971952586338436, Time: 467.49 seconds\n",
      "Epoch 3/50, Loss: 0.3658609827016962, Time: 463.91 seconds\n",
      "Epoch 4/50, Loss: 0.36551211346154927, Time: 464.19 seconds\n",
      "Epoch 5/50, Loss: 0.36311823797637016, Time: 463.85 seconds\n",
      "Epoch 6/50, Loss: 0.3655607434867442, Time: 464.86 seconds\n",
      "Epoch 7/50, Loss: 0.3631459799991257, Time: 465.20 seconds\n",
      "Epoch 8/50, Loss: 0.36492481776352587, Time: 463.71 seconds\n",
      "Epoch 9/50, Loss: 0.36193369054931335, Time: 465.01 seconds\n",
      "Epoch 10/50, Loss: 0.3631024405188944, Time: 463.43 seconds\n",
      "Epoch 11/50, Loss: 0.3643462036190362, Time: 465.11 seconds\n",
      "Epoch 12/50, Loss: 0.36584447992259056, Time: 463.78 seconds\n",
      "Epoch 13/50, Loss: 0.3625232120012415, Time: 464.64 seconds\n",
      "Epoch 14/50, Loss: 0.36582015831580106, Time: 464.46 seconds\n",
      "Epoch 15/50, Loss: 0.36143635641569377, Time: 465.06 seconds\n",
      "Epoch 16/50, Loss: 0.365202509466259, Time: 464.46 seconds\n",
      "Epoch 17/50, Loss: 0.36200880747416925, Time: 464.42 seconds\n",
      "Epoch 18/50, Loss: 0.3602999923215515, Time: 465.05 seconds\n",
      "Epoch 19/50, Loss: 0.36465455917106276, Time: 465.22 seconds\n",
      "Epoch 20/50, Loss: 0.3622226995983343, Time: 464.39 seconds\n",
      "Epoch 21/50, Loss: 0.3632521728674571, Time: 464.90 seconds\n",
      "Epoch 22/50, Loss: 0.3637517610158043, Time: 464.00 seconds\n",
      "Epoch 23/50, Loss: 0.36295705930940036, Time: 465.89 seconds\n",
      "Epoch 24/50, Loss: 0.36404193372562016, Time: 463.43 seconds\n",
      "Epoch 25/50, Loss: 0.3615956136892582, Time: 465.02 seconds\n",
      "Epoch 26/50, Loss: 0.3628035490882808, Time: 464.87 seconds\n",
      "Epoch 27/50, Loss: 0.3642620038369606, Time: 465.25 seconds\n",
      "Epoch 28/50, Loss: 0.3628867823502113, Time: 465.22 seconds\n",
      "Epoch 29/50, Loss: 0.3629099557112003, Time: 464.85 seconds\n",
      "Epoch 30/50, Loss: 0.35993896561792527, Time: 466.05 seconds\n",
      "Epoch 31/50, Loss: 0.36237508658019973, Time: 484.01 seconds\n",
      "Epoch 32/50, Loss: 0.36411719897697714, Time: 467.09 seconds\n",
      "Epoch 33/50, Loss: 0.36497807177318925, Time: 466.26 seconds\n",
      "Epoch 34/50, Loss: 0.35954910328333406, Time: 466.00 seconds\n",
      "Epoch 35/50, Loss: 0.361170990371156, Time: 466.07 seconds\n",
      "Epoch 36/50, Loss: 0.36410677433013916, Time: 466.16 seconds\n",
      "Epoch 37/50, Loss: 0.3616019930990263, Time: 465.74 seconds\n",
      "Epoch 38/50, Loss: 0.36607801897087316, Time: 466.23 seconds\n",
      "Epoch 39/50, Loss: 0.36117443442344666, Time: 466.04 seconds\n",
      "Epoch 40/50, Loss: 0.36424342020489703, Time: 467.06 seconds\n",
      "Epoch 41/50, Loss: 0.3625739975564781, Time: 466.66 seconds\n",
      "Epoch 42/50, Loss: 0.3621864952575201, Time: 466.64 seconds\n",
      "Epoch 43/50, Loss: 0.3608539633709809, Time: 465.62 seconds\n",
      "Epoch 44/50, Loss: 0.36067099286907023, Time: 467.64 seconds\n",
      "Epoch 45/50, Loss: 0.36277083453090714, Time: 465.83 seconds\n",
      "Epoch 46/50, Loss: 0.361950457609933, Time: 465.44 seconds\n",
      "Epoch 47/50, Loss: 0.36441262032108745, Time: 465.86 seconds\n",
      "Epoch 48/50, Loss: 0.36161685846317776, Time: 466.18 seconds\n",
      "Epoch 49/50, Loss: 0.36256439350117214, Time: 466.31 seconds\n",
      "Epoch 50/50, Loss: 0.36412716391442834, Time: 466.24 seconds\n",
      "Finished Training in 23313.69 seconds\n",
      "Model saved to cnn_lstm_model.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.40370816, 0.10678113, 0.03576658, 0.0974915 ], dtype=float32)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train\n",
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "import time\n",
    "\n",
    "# 특정 키들\n",
    "valid_keys = ['w', 'a', 's', 'd']\n",
    "special_labels = ['no_keys_pressed']\n",
    "\n",
    "# 데이터셋 클래스 정의\n",
    "class KeypressDataset(Dataset):\n",
    "    def __init__(self, base_folders, sequence_length=10, transform=None):\n",
    "        self.transform = transform\n",
    "        self.sequence_length = sequence_length\n",
    "        self.image_sequences = []\n",
    "        self.label_sequences = []\n",
    "        self.speed_sequences = []\n",
    "\n",
    "        for base_folder in base_folders:\n",
    "            speed_file_path = os.path.join(base_folder, 'text', 'speed.txt')\n",
    "            with open(speed_file_path, 'r') as f:\n",
    "                speed_values = [float(line.strip()) for line in f]\n",
    "\n",
    "            label_folders = [f for f in os.listdir(base_folder) if os.path.isdir(os.path.join(base_folder, f)) and self.is_valid_label(f)]\n",
    "\n",
    "            all_image_paths = []\n",
    "            for label_folder in label_folders:\n",
    "                folder_path = os.path.join(base_folder, label_folder)\n",
    "                image_paths = glob.glob(os.path.join(folder_path, '*.png'))\n",
    "                for img_path in image_paths:\n",
    "                    all_image_paths.append((img_path, label_folder))\n",
    "\n",
    "            all_image_paths.sort(key=lambda x: int(os.path.basename(x[0]).split('.')[0]))\n",
    "\n",
    "            for i in range(len(all_image_paths) - self.sequence_length + 1):\n",
    "                image_sequence = []\n",
    "                label_sequence = []\n",
    "                speed_sequence = []\n",
    "\n",
    "                for j in range(self.sequence_length):\n",
    "                    img_path, label_folder = all_image_paths[i + j]\n",
    "                    label_array = self.label_to_array(label_folder)\n",
    "                    image_sequence.append(img_path)\n",
    "                    label_sequence.append(label_array)\n",
    "                    speed_sequence.append(speed_values[i + j])\n",
    "\n",
    "                if len(image_sequence) == self.sequence_length:\n",
    "                    self.image_sequences.append(image_sequence)\n",
    "                    self.label_sequences.append(label_sequence)\n",
    "                    self.speed_sequences.append(speed_sequence)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_sequence_paths = self.image_sequences[idx]\n",
    "        label_sequence = self.label_sequences[idx]\n",
    "        speed_sequence = self.speed_sequences[idx]\n",
    "\n",
    "        image_sequence = []\n",
    "        for image_path in image_sequence_paths:\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "            image = np.array(image)\n",
    "            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "            edges = cv2.Canny(gray, 50, 150)\n",
    "            image = Image.fromarray(edges).convert('L')\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            image_sequence.append(image)\n",
    "\n",
    "        image_sequence = torch.stack(image_sequence)\n",
    "        label_sequence = torch.tensor(label_sequence, dtype=torch.float)\n",
    "        speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        return image_sequence, label_sequence, speed_sequence\n",
    "\n",
    "    def is_valid_label(self, label):\n",
    "        if label in special_labels:\n",
    "            return True\n",
    "        for char in label.split('_'):\n",
    "            if char not in valid_keys:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def label_to_array(self, label):\n",
    "        if label == 'no_keys_pressed':\n",
    "            return [0, 0, 0, 0]\n",
    "        array = [0, 0, 0, 0]\n",
    "        for char in label.split('_'):\n",
    "            if char in valid_keys:\n",
    "                index = valid_keys.index(char)\n",
    "                array[index] = 1\n",
    "        return array\n",
    "\n",
    "# 데이터 변환 정의\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=256, lstm_hidden_size=128, sequence_length=10):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.cnn_output_size = cnn_output_size\n",
    "\n",
    "        # CNN 모델\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, cnn_output_size, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(cnn_output_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully Connected 레이어를 추가하여 CNN 출력 크기를 줄임\n",
    "        self.fc1 = nn.Linear(cnn_output_size * 8 * 8, 512)  # 8*8은 임의의 값, 실제로 확인해야 함\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "        # LSTM 모델\n",
    "        self.lstm = nn.LSTM(input_size=256 + 1, hidden_size=lstm_hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc3 = nn.Linear(lstm_hidden_size, 4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, speed_sequence):\n",
    "        # Check if input x has the expected shape\n",
    "        if len(x.size()) == 5:\n",
    "            batch_size, seq_len, c, h, w = x.size()\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input shape: {x.size()}\")\n",
    "\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            x_t = self.pool(self.relu(self.bn1(self.conv1(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn2(self.conv2(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn3(self.conv3(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn4(self.conv4(x_t))))\n",
    "            x_t = x_t.view(batch_size, -1)\n",
    "            x_t = self.relu(self.fc1(x_t))\n",
    "            x_t = self.relu(self.fc2(x_t))\n",
    "            cnn_features.append(x_t)\n",
    "\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "        lstm_input = torch.cat((cnn_features, speed_sequence), dim=2)\n",
    "\n",
    "\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "num_epochs = 50\n",
    "\n",
    "def train_model(base_folders, batch_size=16):\n",
    "    dataset = KeypressDataset(base_folders=base_folders, transform=transform, sequence_length=10)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    model = CNNLSTMModel().cuda()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    torch.cuda.empty_cache()\n",
    "    start_time = time.time()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        epoch_start_time = time.time()\n",
    "        for images, labels, speeds in dataloader:\n",
    "            images = images.cuda()\n",
    "            labels = labels.cuda()\n",
    "            speeds = speeds.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images, speeds)\n",
    "            loss = criterion(outputs, labels[:, -1, :])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        epoch_end_time = time.time()\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(dataloader)}, Time: {epoch_end_time - epoch_start_time:.2f} seconds')\n",
    "    total_time = time.time() - start_time\n",
    "    print(f'Finished Training in {total_time:.2f} seconds')\n",
    "    torch.save(model.state_dict(), 'cnn_lstm_model.pth')\n",
    "    print('Model saved to cnn_lstm_model.pth')\n",
    "\n",
    "def predict_image_sequence(image_sequence, speed_sequence, model):\n",
    "    image_sequence = torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)).convert('L')).unsqueeze(0) for image in image_sequence])\n",
    "    image_sequence = image_sequence.unsqueeze(0)\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence.cuda(), speed_sequence.cuda())\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def load_model_and_predict(image_sequence, speed_sequence):\n",
    "    # image_sequence = torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)).convert('L')).unsqueeze(0) for image in image_sequence])\n",
    "    image_sequence = image_sequence.unsqueeze(0)  # 배치 차원 추가\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)  # 배치 차원 및 채널 차원 추가\n",
    "\n",
    "    model = CNNLSTMModel()\n",
    "    model.load_state_dict(torch.load('cnn_lstm_model.pth'))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence.cuda(), speed_sequence.cuda())\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    current_directory = os.getcwd()\n",
    "    base_folders = [os.path.join(current_directory, name) for name in os.listdir(current_directory) if os.path.isdir(os.path.join(current_directory, name))]\n",
    "    train_model(base_folders, batch_size=16)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example_image_sequence_paths = [\n",
    "    'screenshots_20240608-050128/no_keys_pressed/1.png',\n",
    "    'screenshots_20240608-050128/w/2.png',\n",
    "    'screenshots_20240608-050128/w/3.png',\n",
    "    'screenshots_20240608-050128/d/4.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/5.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/6.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/7.png',\n",
    "    'screenshots_20240608-050128/w/8.png',\n",
    "    'screenshots_20240608-050128/w/9.png',\n",
    "    'screenshots_20240608-050128/w/10.png'\n",
    "]\n",
    "example_speed_sequence = [20, 36, 38, 36, 48, 47, 60, 75, 91, 106]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_image_sequence = []\n",
    "for image_path in example_image_sequence_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    image = Image.fromarray(edges).convert('L')\n",
    "    image = transform(image)\n",
    "    test_image_sequence.append(image)\n",
    "\n",
    "test_image_sequence = torch.stack(test_image_sequence)\n",
    "\n",
    "\n",
    "# 모델 예측 수행\n",
    "load_model_and_predict(test_image_sequence, example_speed_sequence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: torch.Size([1, 10, 257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.34630337, 0.08237204, 0.02891213, 0.09818259], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test\n",
    "def load_model_and_predict(image_sequence, speed_sequence):\n",
    "    # image_sequence = torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)).convert('L')).unsqueeze(0) for image in image_sequence])\n",
    "    image_sequence = image_sequence.unsqueeze(0)  # 배치 차원 추가\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)  # 배치 차원 및 채널 차원 추가\n",
    "\n",
    "    model = CNNLSTMModel()\n",
    "    model.load_state_dict(torch.load('cnn_lstm_model.pth'))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence.cuda(), speed_sequence.cuda())\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "    \n",
    "example_image_sequence_paths = [\n",
    "    'screenshots_20240608-050128/no_keys_pressed/1.png',\n",
    "    'screenshots_20240608-050128/w/2.png',\n",
    "    'screenshots_20240608-050128/w/3.png',\n",
    "    'screenshots_20240608-050128/d/4.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/5.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/6.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/7.png',\n",
    "    'screenshots_20240608-050128/w/8.png',\n",
    "    'screenshots_20240608-050128/w/9.png',\n",
    "    'screenshots_20240608-050128/w/10.png'\n",
    "]\n",
    "example_speed_sequence = [20, 36, 38, 36, 48, 47, 60, 75, 91, 106]\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "test_image_sequence = []\n",
    "for image_path in example_image_sequence_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = np.array(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    image = Image.fromarray(edges).convert('L')\n",
    "    image = transform(image)\n",
    "    test_image_sequence.append(image)\n",
    "\n",
    "test_image_sequence = torch.stack(test_image_sequence)\n",
    "\n",
    "\n",
    "# 모델 예측 수행\n",
    "load_model_and_predict(test_image_sequence, example_speed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM input shape: torch.Size([1, 10, 257])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.40370816, 0.10678113, 0.03576658, 0.0974915 ], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0611\n",
    "def load_model_and_predict(image_sequence, speed_sequence):\n",
    "    # image_sequence = torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)).convert('L')).unsqueeze(0) for image in image_sequence])\n",
    "    image_sequence = image_sequence.unsqueeze(0)  # 배치 차원 추가\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)  # 배치 차원 및 채널 차원 추가\n",
    "\n",
    "    model = CNNLSTMModel()\n",
    "    model.load_state_dict(torch.load('cnn_lstm_model.pth'))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence.cuda(), speed_sequence.cuda())\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def image_processing(image):\n",
    "    transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    \n",
    "    image = np.array(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    image = Image.fromarray(edges).convert('L')\n",
    "    image = transform(image)\n",
    "\n",
    "    return image\n",
    "    \n",
    "example_image_sequence_paths = [\n",
    "    'screenshots_20240608-050128/no_keys_pressed/1.png',\n",
    "    'screenshots_20240608-050128/w/2.png',\n",
    "    'screenshots_20240608-050128/w/3.png',\n",
    "    'screenshots_20240608-050128/d/4.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/5.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/6.png',\n",
    "    'screenshots_20240608-050128/no_keys_pressed/7.png',\n",
    "    'screenshots_20240608-050128/w/8.png',\n",
    "    'screenshots_20240608-050128/w/9.png',\n",
    "    'screenshots_20240608-050128/w/10.png'\n",
    "]\n",
    "example_speed_sequence = [20, 36, 38, 36, 48, 47, 60, 75, 91, 106]\n",
    "\n",
    "\n",
    "\n",
    "test_image_sequence = []\n",
    "for image_path in example_image_sequence_paths:\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    image = image_processing(image)\n",
    "\n",
    "    test_image_sequence.append(image)\n",
    "\n",
    "test_image_sequence = torch.stack(test_image_sequence)\n",
    "# print(test_image_sequence)\n",
    "\n",
    "# 모델 예측 수행\n",
    "load_model_and_predict(test_image_sequence, example_speed_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 97ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 23, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 30, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 28, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 26, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 23, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 22, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 18, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 17, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 19, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 18, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 24, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 30, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 35, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 36, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 33, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 31, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 30, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 28, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 26, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 23, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 22, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 20, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 20, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 20, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 23, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 27, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 27, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 25, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 23, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 22, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 18, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 17, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 75, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 12, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 70, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 70, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 7, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 70, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 14, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 6, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 27, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 33, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 36, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 36, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 41, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 39, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 38, Keys: \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
      "LSTM input shape: torch.Size([1, 10, 257])\n",
      "Predicted keys: [0.40370816 0.10678113 0.03576658 0.0974915 ]\n",
      "Speed: 36, Keys: \n"
     ]
    }
   ],
   "source": [
    "# eval\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "from mss import mss\n",
    "from pynput import keyboard\n",
    "from keras.models import load_model\n",
    "from pynput.keyboard import Key, Controller, Listener\n",
    "\n",
    "# 특정 키들\n",
    "valid_keys = ['w', 'a', 's', 'd']\n",
    "special_labels = ['no_keys_pressed']\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "# ESC 키 종료 플래그\n",
    "exit_flag = False\n",
    "\n",
    "def on_press(key):\n",
    "    global exit_flag\n",
    "    if key == Key.esc:\n",
    "        exit_flag = True\n",
    "        return False\n",
    "\n",
    "class CNNLSTMModel(nn.Module):\n",
    "    def __init__(self, cnn_output_size=256, lstm_hidden_size=128, sequence_length=10):\n",
    "        super(CNNLSTMModel, self).__init__()\n",
    "        self.sequence_length = sequence_length\n",
    "        self.cnn_output_size = cnn_output_size\n",
    "\n",
    "        # CNN 모델\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, cnn_output_size, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(cnn_output_size)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "\n",
    "        # Fully Connected 레이어를 추가하여 CNN 출력 크기를 줄임\n",
    "        self.fc1 = nn.Linear(cnn_output_size * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "\n",
    "        # LSTM 모델\n",
    "        self.lstm = nn.LSTM(input_size=256 + 1, hidden_size=lstm_hidden_size, num_layers=2, batch_first=True)\n",
    "        self.fc3 = nn.Linear(lstm_hidden_size, 4)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, speed_sequence):\n",
    "        batch_size, seq_len, c, h, w = x.size()\n",
    "        cnn_features = []\n",
    "        for t in range(seq_len):\n",
    "            x_t = x[:, t, :, :, :]\n",
    "            x_t = self.pool(self.relu(self.bn1(self.conv1(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn2(self.conv2(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn3(self.conv3(x_t))))\n",
    "            x_t = self.pool(self.relu(self.bn4(self.conv4(x_t))))\n",
    "            x_t = x_t.view(batch_size, -1)\n",
    "            x_t = self.relu(self.fc1(x_t))\n",
    "            x_t = self.relu(self.fc2(x_t))\n",
    "            cnn_features.append(x_t)\n",
    "\n",
    "        cnn_features = torch.stack(cnn_features, dim=1)\n",
    "        lstm_input = torch.cat((cnn_features, speed_sequence), dim=2)\n",
    "        \n",
    "        # LSTM 입력 크기 확인\n",
    "        print(f\"LSTM input shape: {lstm_input.shape}\")\n",
    "\n",
    "        lstm_out, _ = self.lstm(lstm_input)\n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        out = self.dropout(lstm_out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "def predict_image_sequence(image_sequence, speed_sequence, model):\n",
    "    image_sequence = torch.stack([transform(Image.fromarray(cv2.cvtColor(np.array(image), cv2.COLOR_BGR2GRAY)).convert('L')).unsqueeze(0) for image in image_sequence])\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence, speed_sequence)\n",
    "        output = output.squeeze().numpy()\n",
    "    return output\n",
    "\n",
    "def load_model_and_predict(image_sequence, speed_sequence):\n",
    "    image_sequence = torch.stack(image_sequence).unsqueeze(0)  # 배치 차원 추가\n",
    "    speed_sequence = torch.tensor(speed_sequence, dtype=torch.float).unsqueeze(0).unsqueeze(2)\n",
    "\n",
    "    model = CNNLSTMModel()\n",
    "    model.load_state_dict(torch.load('cnn_lstm_model.pth'))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(image_sequence.cuda(), speed_sequence.cuda())\n",
    "        output = output.squeeze().cpu().numpy()\n",
    "    return output\n",
    "\n",
    "def capture_screen(bbox):\n",
    "    with mss() as sct:\n",
    "        screenshot = sct.grab(bbox)\n",
    "        img = Image.frombytes('RGB', (screenshot.width, screenshot.height), screenshot.rgb)\n",
    "        return img\n",
    "\n",
    "def get_speed(speed_image, speed_model):\n",
    "    speed_image = cv2.cvtColor(speed_image, cv2.COLOR_BGR2GRAY)\n",
    "    speed_image = speed_image.astype('float32') / 255\n",
    "    speed_image = np.expand_dims(speed_image, axis=0)\n",
    "    speed_image = np.expand_dims(speed_image, axis=-1)\n",
    "    prediction = speed_model.predict(speed_image)\n",
    "    predicted_label = np.argmax(prediction, axis=1)[0]\n",
    "    return predicted_label\n",
    "\n",
    "def image_processing(image):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((128, 128)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "    ])\n",
    "    image = np.array(image)\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    edges = cv2.Canny(gray, 50, 150)\n",
    "    image = Image.fromarray(edges).convert('L')\n",
    "    image = transform(image)\n",
    "    return image\n",
    "\n",
    "def main():\n",
    "    global exit_flag\n",
    "    game_bbox = {'top': 170, 'left': 320, 'width': 1330, 'height': 910}\n",
    "    speed_bbox = {'top': 905, 'left': 145, 'width': 202 - 145, 'height': 940 - 905}\n",
    "    \n",
    "    # 키보드 컨트롤러\n",
    "    keyboard_controller = Controller()\n",
    "\n",
    "    speed_model = load_model('speed_rec.h5')\n",
    "    model = CNNLSTMModel()\n",
    "    model.load_state_dict(torch.load('cnn_lstm_model.pth'))\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    \n",
    "    image_sequence = []\n",
    "    speed_sequence = []\n",
    "\n",
    "    listener = Listener(on_press=on_press)\n",
    "    listener.start()\n",
    "\n",
    "    while not exit_flag:\n",
    "        img = capture_screen(game_bbox)\n",
    "        image_tensor = image_processing(img)\n",
    "        image_sequence.append(image_tensor)\n",
    "        \n",
    "        speed_screen = capture_screen(speed_bbox)\n",
    "        speed_screen = np.array(speed_screen)\n",
    "        speed_screen = cv2.cvtColor(speed_screen, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        speed = get_speed(speed_screen, speed_model)\n",
    "        speed_sequence.append(speed)\n",
    "\n",
    "        if len(image_sequence) == 10:\n",
    "            prediction = load_model_and_predict(image_sequence, speed_sequence)\n",
    "            print(f'Predicted keys: {prediction}')\n",
    "            image_sequence.pop(0)\n",
    "            speed_sequence.pop(0)\n",
    "\n",
    "            # 예측 확률에 따른 키 입력\n",
    "            pressed_keys = []\n",
    "            for i, prob in enumerate(prediction):\n",
    "                if prob >= 0.7:\n",
    "                    key = valid_keys[i]\n",
    "                    pressed_keys.append(key)\n",
    "                    keyboard_controller.press(key)\n",
    "                else:\n",
    "                    key = valid_keys[i]\n",
    "                    keyboard_controller.release(key)\n",
    "\n",
    "            # 콘솔에 속도 값과 입력된 키 표시\n",
    "            print(f'Speed: {speed}, Keys: {\" \".join(pressed_keys)}')\n",
    "\n",
    "        if exit_flag:\n",
    "            listener.stop()\n",
    "            break\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
